# ğŸ¤– IA de ManutenÃ§Ã£o Industrial (LLM Local)

Este projeto implementa uma **IA de teste para apoio Ã  manutenÃ§Ã£o industrial**, executada **localmente**, utilizando um **LLM openâ€‘source**, **manuais tÃ©cnicos em PDF** (RAG) e **busca na web via Tavily** para complementar as respostas.

O objetivo Ã© servir como **prova de conceito** para avaliar o uso de LLMs na anÃ¡lise de falhas, manutenÃ§Ã£o preventiva/corretiva e apoio tÃ©cnico a engenheiros.

---

## ğŸ§  VisÃ£o Geral

A IA Ã© capaz de:

* ğŸ“„ Ler e indexar **manuais tÃ©cnicos em PDF**
* ğŸ” Recuperar informaÃ§Ãµes relevantes via **RAG (Retrievalâ€‘Augmented Generation)**
* ğŸŒ Complementar respostas com **busca na web (Tavily Search)**
* ğŸ¤– Executar um **LLM local (Mistral 7B Instruct via Ollama)**
* ğŸ–¥ï¸ Rodar localmente (offline, exceto web search)

---

## ğŸ—ï¸ Arquitetura

```
UsuÃ¡rio
  â†“
Pergunta
  â†“
RAG (FAISS + PDFs)
  â†“
LLM Local (Mistral 7B)
  â†“
Resposta tÃ©cnica
  +
Complemento via Tavily (Web)
```

---

## ğŸ“ Estrutura do Projeto

```
project-root/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py          # Interface CLI
â”‚   â”œâ”€â”€ rag_chain.py     # RAG (LCEL)
â”‚   â”œâ”€â”€ llm.py           # ConexÃ£o com LLM local
â”‚   â””â”€â”€ ingest.py        # IngestÃ£o e indexaÃ§Ã£o dos PDFs
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ pdfs/            # Manuais tÃ©cnicos
â”‚   â””â”€â”€ vectorstore/     # Ãndice FAISS
â”‚
â”œâ”€â”€ .env                 # Chave da Tavily
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Requisitos

* Python **3.10+**
* Ollama (LLM local)
* CPU ou GPU (quantizaÃ§Ã£o permite uso em mÃ¡quinas modestas)

---

## ğŸš€ InstalaÃ§Ã£o

### 1ï¸âƒ£ Criar ambiente virtual

```bash
python -m venv venv
venv\Scripts\activate   # Windows
# ou
source venv/bin/activate # Linux
```

---

### 2ï¸âƒ£ Instalar dependÃªncias

```bash
pip install -U \
langchain-core \
langchain-community \
langchain-huggingface \
langchain-text-splitters \
faiss-cpu \
sentence-transformers \
pypdf \
python-dotenv \
tavily-python
```

---

### 3ï¸âƒ£ Instalar e rodar o LLM local (Ollama)

1. Instale o Ollama: [https://ollama.com](https://ollama.com)
2. Baixe o modelo:

```bash
ollama pull mistral:7b-instruct
```

3. Inicie o servidor:

```bash
ollama serve
```

---

## ğŸ“„ IngestÃ£o de PDFs (RAG)

1. Coloque os manuais tÃ©cnicos em:

```
data/pdfs/
```

âš ï¸ Os PDFs devem conter **texto selecionÃ¡vel** (PDFs escaneados exigem OCR).

2. Execute a indexaÃ§Ã£o:

```bash
python app/ingest.py
```

Isso criarÃ¡ o Ã­ndice vetorial em `data/vectorstore/`.

---

## ğŸŒ ConfiguraÃ§Ã£o da Busca Web (Tavily)

Crie um arquivo `.env` na raiz do projeto:

```env
TAVILY_API_KEY=SUA_CHAVE_AQUI
```

A chave pode ser obtida em: [https://tavily.com](https://tavily.com)

---

## â–¶ï¸ Executar a IA

Sempre execute a partir da **raiz do projeto**:

```bash
python -m app.main
```

---

## ğŸ§ª Exemplo de Uso

```
Pergunta:
Quais sÃ£o as causas de vibraÃ§Ã£o excessiva em uma mÃ¡quina industrial?
```

A resposta serÃ¡:

* ğŸ“„ Baseada nos **manuais tÃ©cnicos**
* ğŸ” Complementada com **informaÃ§Ãµes recentes da web**

---

## ğŸ§© Tecnologias Utilizadas

* **Python**
* **LangChain (LCEL)**
* **FAISS** (vetorizaÃ§Ã£o local)
* **Sentenceâ€‘Transformers** (embeddings)
* **Ollama** (LLM local)
* **Mistral 7B Instruct** (openâ€‘source)
* **Tavily Search** (busca web)

---

## ğŸ”® PrÃ³ximos Passos (EvoluÃ§Ã£o)

* [ ] Unificar resposta de PDFs + Web em um Ãºnico output
* [ ] Criar prompt system profissional (engenharia de manutenÃ§Ã£o)
* [ ] GeraÃ§Ã£o automÃ¡tica de relatÃ³rio de manutenÃ§Ã£o
* [ ] API REST com FastAPI
* [ ] Interface web
* [ ] Suporte a OCR para PDFs escaneados

---

## ğŸ“œ LicenÃ§a

Projeto de teste / prova de conceito para fins educacionais e experimentais.
